{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9226a53",
   "metadata": {},
   "source": [
    " # ***ICU-DISPO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f6d95",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.pipeline import make_pipeline  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.ensemble import StackingClassifier \n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, brier_score_loss\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV  \n",
    "from sklearn.isotonic import IsotonicRegression  \n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa84ad",
   "metadata": {},
   "source": [
    "### Import base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059df313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifier, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1862d6",
   "metadata": {},
   "source": [
    "### `data＿loading` \n",
    "This function loads a csv. file, splits it into training and testing set (default test size = 0.3), and returns the train and test dictionary with its outcome (Y), treatment (A), covariates (W) and covariates & teatment(W_A) .It also prints a summary of the split, including sample counts and treatment distribution, to help verify balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(file_path, test_size=0.3, random_state=42):\n",
    "        df = pd.read_csv(file_path)\n",
    "        Y = df[\"Y\"].values\n",
    "        A = df[\"A\"].values\n",
    "        W = df.drop(columns=[\"Y\", \"A\"])\n",
    "        W_A = df.drop(columns=[\"Y\"])\n",
    "        \n",
    "        indices = np.arange(len(Y))\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            indices, test_size=test_size, random_state=random_state, \n",
    "            stratify=A\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Data Split Summary:\")\n",
    "        print(f\"   Total samples: {len(Y)}\")\n",
    "        print(f\"   Train samples: {len(train_idx)} ({len(train_idx)/len(Y)*100:.1f}%)\")\n",
    "        print(f\"   Test samples: {len(test_idx)} ({len(test_idx)/len(Y)*100:.1f}%)\")\n",
    "        print(f\"   Train treatment prop: {A[train_idx].mean():.3f}\")\n",
    "        print(f\"   Test treatment prop: {A[test_idx].mean():.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'train': {\n",
    "                'Y': Y[train_idx], 'A': A[train_idx], \n",
    "                'W': W.iloc[train_idx], 'W_A': W_A.iloc[train_idx],\n",
    "                'indices': train_idx\n",
    "            },\n",
    "            'test': {\n",
    "                'Y': Y[test_idx], 'A': A[test_idx],\n",
    "                'W': W.iloc[test_idx], 'W_A': W_A.iloc[test_idx], \n",
    "                'indices': test_idx\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbeabb",
   "metadata": {},
   "source": [
    "### `data_preprocessing `\n",
    "This function performs preprocessing on training (and optionally testing) feature datasets.\n",
    "It handles missing values and applies standardization (Z-score standardization) so the features are suitable for modeling.\n",
    "The same scaling is applied consistently to both training and test sets to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(W_train, W_test=None):\n",
    "\n",
    "        # Determine W_train type\n",
    "        if isinstance(W_train, pd.DataFrame):\n",
    "            median_train = W_train.median()\n",
    "        else:\n",
    "            median_train = np.median(W_train, axis=0)\n",
    "        # Data cleaning: fill NaN values with median\n",
    "        if isinstance(W_train, pd.DataFrame):\n",
    "            W_train_clean = W_train.fillna(median_train)\n",
    "        else:\n",
    "            W_train_clean = np.where(np.isnan(W_train), median_train, W_train)\n",
    "        # ======================================================================   \n",
    "        scaler = StandardScaler()\n",
    "        W_train_standardized = scaler.fit_transform(W_train_clean)\n",
    "        # ======================================================================\n",
    "        if isinstance(W_train, pd.DataFrame):\n",
    "            W_train_df = pd.DataFrame(W_train_standardized, columns=W_train.columns)\n",
    "        else:\n",
    "            W_train_df = pd.DataFrame(W_train_standardized)\n",
    "        \n",
    "        if W_test is not None:\n",
    "            if isinstance(W_test, pd.DataFrame):\n",
    "                W_test_clean = W_test.fillna(median_train)\n",
    "            else:\n",
    "                W_test_clean = np.where(np.isnan(W_test), median_train, W_test)\n",
    "            W_test_standardized = scaler.transform(W_test_clean)\n",
    "            if isinstance(W_test, pd.DataFrame):\n",
    "                W_test_df = pd.DataFrame(W_test_standardized, columns=W_test.columns)\n",
    "            else:\n",
    "                W_test_df = pd.DataFrame(W_test_standardized)\n",
    "            return W_train_df, W_test_df, scaler\n",
    "        return W_train_df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe771809",
   "metadata": {},
   "source": [
    "### `get_base_learner `\n",
    "- Linear Models\n",
    "    - logistic_cv: Logistic regression with cross-validation (roc_auc scoring, balanced class weights).\n",
    "    - logistic_l1: L1-regularized logistic regression (sparse features).\n",
    "    - logistic_elastic: Logistic regression with elastic-net regularization (L1+L2).\n",
    "\n",
    "- Tree-Based Models\n",
    "    - rf: Random Forest with 150 trees, max depth = 10, class balancing.\n",
    "    - extra_trees: Extremely randomized trees, shallower than RF.\n",
    "    - gbm: Gradient Boosting (learning_rate=0.08, subsample=0.8, early stopping).\n",
    "    - xgb: XGBoost classifier with regularization (reg_alpha, reg_lambda).\n",
    "    - lgbm: LightGBM with class balancing and shrinkage parameters.\n",
    "\n",
    "- Non-Linear Models\n",
    "    - svm_rbf: Support Vector Machine with RBF kernel (scaled features).\n",
    "    - svm_linear: Linear kernel SVM with regularization C=0.1.\n",
    "    - mlp: Multi-layer Perceptron (3 hidden layers: 50-30-15, adaptive learning rate, early stopping).\n",
    "\n",
    "- Simple Models\n",
    "    - nb: Gaussian Naive Bayes (with feature scaling).\n",
    "    - knn: k-Nearest Neighbors (k=15, distance weighting).\n",
    "    - dt: Decision Tree (depth-limited, class-balanced).\n",
    "return a list of base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f337a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_learners():\n",
    "\n",
    "        base_learners = [\n",
    "            # ========== LINEAR MODELS ==========\n",
    "            ('logistic_cv', LogisticRegressionCV(\n",
    "                cv=5, \n",
    "                max_iter=10000, \n",
    "                random_state=42,\n",
    "                solver='lbfgs',\n",
    "                scoring='roc_auc',\n",
    "                class_weight='balanced'\n",
    "            )),\n",
    "            \n",
    "            ('logistic_l1', LogisticRegressionCV(\n",
    "                cv=5, \n",
    "                max_iter=10000, \n",
    "                penalty='l1',\n",
    "                solver='liblinear', \n",
    "                random_state=42,\n",
    "                tol=1e-4,\n",
    "                scoring='roc_auc',\n",
    "                class_weight='balanced'\n",
    "            )),\n",
    "            \n",
    "            ('logistic_elastic', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                LogisticRegressionCV(\n",
    "                    cv=5,\n",
    "                    penalty='elasticnet',\n",
    "                    solver='saga',\n",
    "                    l1_ratios=[0.1, 0.5, 0.7, 0.9],\n",
    "                    max_iter=5000,\n",
    "                    random_state=42,\n",
    "                    class_weight='balanced'\n",
    "                )\n",
    "            )),\n",
    "            \n",
    "            # ========== TREE-BASED MODELS ==========\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=150,\n",
    "                max_depth=10,\n",
    "                min_samples_split=15,\n",
    "                min_samples_leaf=8,\n",
    "                max_features='sqrt',\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            \n",
    "            ('extra_trees', ExtraTreesClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                max_features='sqrt',\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            \n",
    "            ('gbm', GradientBoostingClassifier(\n",
    "                n_estimators=150,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.08,\n",
    "                subsample=0.8,\n",
    "                max_features='sqrt',\n",
    "                random_state=42,\n",
    "                validation_fraction=0.1,\n",
    "                n_iter_no_change=10\n",
    "            )),\n",
    "            \n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=150,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.08,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                scale_pos_weight=1,\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            \n",
    "            ('lgbm', lgb.LGBMClassifier(\n",
    "                n_estimators=150,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.08,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                random_state=42,\n",
    "                verbosity=-1,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            )),\n",
    "            \n",
    "            # ========== NON-LINEAR MODELS ==========\n",
    "            ('svm_rbf', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                SVC(\n",
    "                    probability=True,\n",
    "                    kernel='rbf',\n",
    "                    C=1.0,\n",
    "                    gamma='scale',\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42\n",
    "                )\n",
    "            )),\n",
    "            \n",
    "            ('svm_linear', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                SVC(\n",
    "                    probability=True,\n",
    "                    kernel='linear',\n",
    "                    C=0.1,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42\n",
    "                )\n",
    "            )),\n",
    "            \n",
    "            ('mlp', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                MLPClassifier(\n",
    "                    hidden_layer_sizes=(50, 30, 15),\n",
    "                    max_iter=3000,\n",
    "                    alpha=0.01,\n",
    "                    learning_rate='adaptive',\n",
    "                    early_stopping=True,\n",
    "                    validation_fraction=0.1,\n",
    "                    n_iter_no_change=15,\n",
    "                    random_state=42\n",
    "                )\n",
    "            )),\n",
    "            \n",
    "            # ========== SIMPLE MODELS ==========\n",
    "            ('nb', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                GaussianNB()\n",
    "            )),\n",
    "            \n",
    "            ('knn', make_pipeline(\n",
    "                StandardScaler(),\n",
    "                KNeighborsClassifier(\n",
    "                    n_neighbors=15,\n",
    "                    weights='distance',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            )),\n",
    "            \n",
    "            ('dt', DecisionTreeClassifier(\n",
    "                max_depth=6,\n",
    "                min_samples_split=30,\n",
    "                min_samples_leaf=15,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "        return base_learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8aa0b",
   "metadata": {},
   "source": [
    "### `fit_super_learner` \n",
    "This function trains a SuperLearner ensemble model using scikit-learn’s StackingClassifier.\n",
    "It combines a diverse set of base learners with a meta-learner (logistic regression with cross-validation)\n",
    "and reports cross-validation(3-fold) performance for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_superlearner(X, Y, base_learners, model_name=\"SuperLearner\"):\n",
    "        print(f\"\\n << Fitting {model_name} >>\")\n",
    "        \n",
    "        pbar = tqdm(total=len(base_learners) + 2, desc=f\"<< Training {model_name} >>\", leave=False)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "            \n",
    "            meta_learner = LogisticRegressionCV(cv=3, max_iter=5000, \n",
    "                                              random_state=42, solver='lbfgs')\n",
    "            \n",
    "            pbar.set_description(f\"Building {model_name}\")\n",
    "            pbar.update(1)\n",
    "            \n",
    "            sl = StackingClassifier(\n",
    "                estimators=base_learners,\n",
    "                cv=3,\n",
    "                stack_method='predict_proba',\n",
    "                final_estimator=meta_learner,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                pbar.set_description(f\"<< Fitting {model_name} >>   \")\n",
    "                sl.fit(X, Y)\n",
    "                pbar.update(1)\n",
    "\n",
    "                pbar.set_description(f\"<< Evaluating {model_name} >>\")\n",
    "                cv_scores = cross_val_score(sl, X, Y, cv=3, scoring='roc_auc')\n",
    "                print(f\"{model_name} Train CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: {model_name} fitting failed: {str(e)}\")\n",
    "                print(\"Using simplified model...\")\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                sl = LogisticRegression(max_iter=5000, random_state=42)\n",
    "                sl.fit(X, Y)\n",
    "            \n",
    "            finally:\n",
    "                pbar.close()\n",
    "        \n",
    "        return sl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c7f75",
   "metadata": {},
   "source": [
    "### `evaluate_calibration` \n",
    "This function evaluates the calibration quality of a probabilistic classifier.\n",
    "It computes the calibration curve, a weighted calibration error, and the Brier Score.\n",
    "The results indicate whether predicted probabilities are well aligned with observed outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_calibration(y_true, y_prob, n_bins=10):\n",
    "        \n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_true, y_prob, n_bins=n_bins, strategy='uniform'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        # Calibration Error\n",
    "        calibration_error = 0\n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper) \n",
    "            prop_in_bin = in_bin.mean()\n",
    "            \n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = y_true[in_bin].mean()\n",
    "                avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "                calibration_error += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        \n",
    "        # Brier Score\n",
    "        brier_score = brier_score_loss(y_true, y_prob)\n",
    "        \n",
    "        return {\n",
    "            'calibration_error': calibration_error,\n",
    "            'brier_score': brier_score,\n",
    "            'fraction_of_positives': fraction_of_positives,\n",
    "            'mean_predicted_value': mean_predicted_value\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5098ba8",
   "metadata": {},
   "source": [
    "### `calibrate_classifier` \n",
    "This function calibrates a classifier’s probability estimates.\n",
    "Calibration adjusts these outputs so that predicted probabilities better reflect observed frequencies.\n",
    "- Platt Scaling (Sigmoid)\n",
    "    - Fits a logistic regression on the classifier’s decision scores.\n",
    "    - Maps raw scores → probabilities with a sigmoid function.\n",
    "    - Works well with limited data.\n",
    "    - Assumes a logistic relationship, which may be too simple in complex cases.\n",
    "- Isotonic Regression\n",
    "    - Fits a piecewise non-decreasing function.\n",
    "    - Very flexible, can adapt to complex calibration patterns.\n",
    "    - Requires more data → can overfit with small samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_classifier(base_model, X_train, y_train, method='platt'):\n",
    "        \n",
    "        print(f\"   🎯 Calibrating classifier using {method} method...\")\n",
    "        \n",
    "        if method == 'platt':\n",
    "            calibrated_model = CalibratedClassifierCV(\n",
    "                base_model, method='sigmoid', cv=3\n",
    "            )\n",
    "        elif method == 'isotonic':\n",
    "            calibrated_model = CalibratedClassifierCV(\n",
    "                base_model, method='isotonic', cv=3\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ⚠️  Unknown calibration method: {method}. Using Platt scaling.\")\n",
    "            calibrated_model = CalibratedClassifierCV(\n",
    "                base_model, method='sigmoid', cv=3\n",
    "            )\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            calibrated_model.fit(X_train, y_train)\n",
    "        \n",
    "        return calibrated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4dd84",
   "metadata": {},
   "source": [
    "### `predict_Q_models` \n",
    "Compute the Q-model predictions used in causal inference / TMLE:\n",
    "\n",
    "- Q_A: Predicted outcome using the observed treatment \n",
    "- Q_1: Counterfactual prediction if A=1 for everyone.\n",
    "- Q_0: Counterfactual prediction if A=0 for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Q_models(sl, W_A_data, is_test=False):\n",
    "        \n",
    "        data_type = \"test\" if is_test else \"train\"\n",
    "        print(f\"   Predicting Q models on {data_type} set...\")\n",
    "        # Predict Q_A\n",
    "        Q_A = sl.predict_proba(W_A_data)[:, 1]\n",
    "        # Predict Q_1 \n",
    "        W_A1 = W_A_data.copy()\n",
    "        W_A1[\"A\"] = 1  \n",
    "        Q_1 = sl.predict_proba(W_A1)[:, 1]\n",
    "        # Predict Q_0\n",
    "        W_A0 = W_A_data.copy()\n",
    "        W_A0[\"A\"] = 0\n",
    "        Q_0 = sl.predict_proba(W_A0)[:, 1]\n",
    "        \n",
    "        return Q_A, Q_1, Q_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0e89b",
   "metadata": {},
   "source": [
    "### `estimate_g_with_calibration` \n",
    "This function estimates propensity scores (g-model) with the following pipeline:\n",
    "\n",
    "- Downsampling to balance treated vs. control in the training set.\n",
    "- Base g-model training using a SuperLearner ensemble.\n",
    "- Calibration (Platt scaling, isotonic regression, or other method).\n",
    "- Evaluation on both the downsampled training set, the full training set, and the test set.\n",
    "- Overlap weighting computation for both training and test data.\n",
    "- Visualization and diagnostics of score distributions.\n",
    "- Returns the calibrated model, diagnostic metrics, and processed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_g_with_calibration(A_train, W_train_standardized, A_test, W_test_standardized, base_learners, calibration_method='platt'):\n",
    "    \n",
    "        print(f\"\\n << Estimating propensity scores with {calibration_method} calibration >>\")\n",
    "        \n",
    "        # Downsampling to balance treated vs. control in the training set\n",
    "        treated_idx = A_train == 1\n",
    "        control_idx = A_train == 0\n",
    "        W_treated = W_train_standardized[treated_idx]\n",
    "        W_control = W_train_standardized[control_idx]\n",
    "        A_treated = A_train[treated_idx]\n",
    "        A_control = A_train[control_idx]\n",
    "\n",
    "        if len(W_treated) > len(W_control):\n",
    "            W_treated_down = resample(W_treated, replace=False, n_samples=len(W_control), random_state=42)\n",
    "            A_treated_down = resample(A_treated, replace=False, n_samples=len(W_control), random_state=42)\n",
    "\n",
    "            W_down = np.vstack([W_treated_down, W_control])\n",
    "            A_down = np.concatenate([A_treated_down, A_control])\n",
    "        else:\n",
    "            W_control_down = resample(W_control, replace=False, n_samples=len(W_treated), random_state=42)\n",
    "            A_control_down = resample(A_control, replace=False, n_samples=len(W_treated), random_state=42)\n",
    "\n",
    "            W_down = np.vstack([W_treated, W_control_down])\n",
    "            A_down = np.concatenate([A_treated, A_control_down])\n",
    "\n",
    "        print(f\"g-model training samples (downsampled): {W_down.shape}, A=1 proportion: {np.mean(A_down):.2f}\")\n",
    "\n",
    "        # Taining the base propensity score model\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            base_g_model = fit_superlearner(W_down, A_down, base_learners, \"Base Propensity Score Model\")\n",
    "\n",
    "        # calibrate g-model\n",
    "        print(\"   📊 Evaluating calibration before calibration...\")\n",
    "        base_probs_train = base_g_model.predict_proba(W_down)[:, 1] \n",
    "        pre_cal_metrics = evaluate_calibration(A_down, base_probs_train) \n",
    "        print(f\"   Pre-calibration: CE={pre_cal_metrics['calibration_error']:.4f}, Brier={pre_cal_metrics['brier_score']:.4f}\")\n",
    "        \n",
    "        calibrated_g_model = calibrate_classifier(base_g_model, W_down, A_down, calibration_method)\n",
    "        \n",
    "        # evaluate calibration after calibration\n",
    "        cal_probs_train = calibrated_g_model.predict_proba(W_down)[:, 1]\n",
    "        post_cal_metrics = evaluate_calibration(A_down, cal_probs_train)\n",
    "        print(f\"   Post-calibration: CE={post_cal_metrics['calibration_error']:.4f}, Brier={post_cal_metrics['brier_score']:.4f}\")\n",
    "        \n",
    "        improvement_ce = pre_cal_metrics['calibration_error'] - post_cal_metrics['calibration_error']\n",
    "        improvement_brier = pre_cal_metrics['brier_score'] - post_cal_metrics['brier_score']\n",
    "        print(f\"   📈 Improvement: CE Δ={improvement_ce:+.4f}, Brier Δ={improvement_brier:+.4f}\")\n",
    "\n",
    "        # evaluate on full training set\n",
    "        print(\"   📊 Evaluating calibrated model on full training set...\")\n",
    "        g_w_train_full = calibrated_g_model.predict_proba(W_train_standardized)[:, 1]\n",
    "        train_cal_metrics = evaluate_calibration(A_train, g_w_train_full)\n",
    "        \n",
    "        # Predict propensity scores on test set\n",
    "        print(\"   Predicting calibrated propensity scores on test set...\")\n",
    "        g_w_test = calibrated_g_model.predict_proba(W_test_standardized)[:, 1]\n",
    "\n",
    "        # evaluate calibration on test set\n",
    "        test_cal_metrics = evaluate_calibration(A_test, g_w_test)\n",
    "        print(f\"   Test set calibration: CE={test_cal_metrics['calibration_error']:.4f}, Brier={test_cal_metrics['brier_score']:.4f}\")\n",
    "\n",
    "        \n",
    "        #  overlap weights for training set\n",
    "        g_w_train_trimmed = np.clip(g_w_train_full, 0.05, 0.95)\n",
    "        overlap_weights_train = g_w_train_trimmed * (1 - g_w_train_trimmed)\n",
    "        H_overlap_train = A_train * (1 - g_w_train_trimmed) - (1 - A_train) * g_w_train_trimmed\n",
    "        H_1_overlap_train = (1 - g_w_train_trimmed)\n",
    "        H_0_overlap_train = g_w_train_trimmed\n",
    "        \n",
    "        # overlap weights for testing set\n",
    "        g_w_test_trimmed = np.clip(g_w_test, 0.05, 0.95)\n",
    "        overlap_weights_test = g_w_test_trimmed * (1 - g_w_test_trimmed)\n",
    "        H_overlap_test = A_test * (1 - g_w_test_trimmed) - (1 - A_test) * g_w_test_trimmed\n",
    "        H_1_overlap_test = (1 - g_w_test_trimmed)\n",
    "        H_0_overlap_test = g_w_test_trimmed\n",
    "\n",
    "        # 7. DIAGNOSTIC INFORMATION\n",
    "        print(f\"\\n📊 Training Set PS Performance:\")\n",
    "        print(f\"   Propensity Score: min={g_w_train_full.min():.4f}, max={g_w_train_full.max():.4f}, mean={g_w_train_full.mean():.4f}\")\n",
    "        print(f\"   Overlap Weights: min={overlap_weights_train.min():.4f}, max={overlap_weights_train.max():.4f}, mean={overlap_weights_train.mean():.4f}\")\n",
    "        train_good_overlap = np.sum((g_w_train_trimmed >= 0.1) & (g_w_train_trimmed <= 0.9))\n",
    "        print(f\"   Good overlap samples (0.1 ≤ PS ≤ 0.9): {train_good_overlap} ({train_good_overlap/len(g_w_train_trimmed)*100:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"   [Average after downsample+overlap+calibration] g_w_train_trimmed mean: {g_w_train_trimmed.mean():.4f}\")\n",
    "        print(f\"   [Average after downsample+overlap+calibration] overlap_weights_train mean: {overlap_weights_train.mean():.4f}\")\n",
    "\n",
    "        # Detailed training set g-model distribution plot\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(8,5))\n",
    "        # g-model probability distribution for testing set: treated/control\n",
    "        treated_scores = g_w_train_trimmed[A_train == 1]\n",
    "        control_scores = g_w_train_trimmed[A_train == 0]\n",
    "        sns.histplot(treated_scores, bins=30, color='royalblue', label='Treated', kde=True, stat='density', alpha=0.6)\n",
    "        sns.histplot(control_scores, bins=30, color='orange', label='Control', kde=True, stat='density', alpha=0.6)\n",
    "        # Adding mean/median\n",
    "        plt.axvline(g_w_train_trimmed.mean(), color='green', linestyle='--', label=f'Mean: {g_w_train_trimmed.mean():.2f}')\n",
    "        plt.axvline(np.median(g_w_train_trimmed), color='red', linestyle=':', label=f'Median: {np.median(g_w_train_trimmed):.2f}')\n",
    "        plt.title('Training Set Propensity Score Distribution (g_w_train_trimmed)')\n",
    "        plt.xlabel('Propensity Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\n📊 Test Set PS Performance:\")\n",
    "        print(f\"   Propensity Score: min={g_w_test.min():.4f}, max={g_w_test.max():.4f}, mean={g_w_test.mean():.4f}\")\n",
    "        print(f\"   Overlap Weights: min={overlap_weights_test.min():.4f}, max={overlap_weights_test.max():.4f}, mean={overlap_weights_test.mean():.4f}\")\n",
    "        test_good_overlap = np.sum((g_w_test_trimmed >= 0.1) & (g_w_test_trimmed <= 0.9))\n",
    "        print(f\"   Good overlap samples (0.1 ≤ PS ≤ 0.9): {test_good_overlap} ({test_good_overlap/len(g_w_test_trimmed)*100:.1f}%)\")\n",
    "        print(f\"   [Average after overlap+calibration] g_w_test_trimmed mean: {g_w_test_trimmed.mean():.4f}\")\n",
    "        print(f\"   [Average after overlap+calibration] overlap_weights_test mean: {overlap_weights_test.mean():.4f}\")\n",
    "\n",
    "        # g-model probability distribution for testing set:treated/control\n",
    "        plt.figure(figsize=(8,5))\n",
    "        treated_scores_test = g_w_test_trimmed[A_test == 1]\n",
    "        control_scores_test = g_w_test_trimmed[A_test == 0]\n",
    "        sns.histplot(treated_scores_test, bins=30, color='royalblue', label='Treated', kde=True, stat='density', alpha=0.6)\n",
    "        sns.histplot(control_scores_test, bins=30, color='orange', label='Control', kde=True, stat='density', alpha=0.6)\n",
    "        plt.axvline(g_w_test_trimmed.mean(), color='green', linestyle='--', label=f'Mean: {g_w_test_trimmed.mean():.2f}')\n",
    "        plt.axvline(np.median(g_w_test_trimmed), color='red', linestyle=':', label=f'Median: {np.median(g_w_test_trimmed):.2f}')\n",
    "        plt.title('Test Set Propensity Score Distribution (g_w_test_trimmed)')\n",
    "        plt.xlabel('Propensity Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # In order to compare with the original model's test performance, we also evaluate the base model on the test set\n",
    "        base_probs_test = base_g_model.predict_proba(W_test_standardized)[:, 1]\n",
    "        base_test_metrics = evaluate_calibration(A_test, base_probs_test)\n",
    "\n",
    "        \n",
    "        calibration_info = {\n",
    "            'pre_calibration_metrics': pre_cal_metrics,\n",
    "            'post_calibration_metrics': post_cal_metrics,\n",
    "            'train_calibration_metrics': train_cal_metrics,  \n",
    "            'test_calibration_metrics': test_cal_metrics,\n",
    "            'base_test_metrics': base_test_metrics,\n",
    "            'calibration_method': calibration_method,\n",
    "            # Information for training set\n",
    "            'train_ps_scores': g_w_train_full,\n",
    "            'train_overlap_weights': overlap_weights_train,\n",
    "            # Mean values for training and testing set\n",
    "            'g_w_train_trimmed_mean': g_w_train_trimmed.mean(),\n",
    "            'overlap_weights_train_mean': overlap_weights_train.mean(),\n",
    "            'g_w_test_trimmed_mean': g_w_test_trimmed.mean(),\n",
    "            'overlap_weights_test_mean': overlap_weights_test.mean()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'model': calibrated_g_model,\n",
    "            'test': {\n",
    "                'g_w': g_w_test_trimmed,\n",
    "                'H_1': H_1_overlap_test,\n",
    "                'H_0': H_0_overlap_test,\n",
    "                'H_overlap': H_overlap_test,\n",
    "                'overlap_weights': overlap_weights_test\n",
    "            },\n",
    "            'train': {  # Information for training set\n",
    "                'g_w': g_w_train_trimmed,\n",
    "                'H_1': H_1_overlap_train,\n",
    "                'H_0': H_0_overlap_train,\n",
    "                'H_overlap': H_overlap_train,\n",
    "                'overlap_weights': overlap_weights_train\n",
    "            },\n",
    "            'calibration_info': calibration_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32233a4",
   "metadata": {},
   "source": [
    "### ` estimate_fluctuation_param `\n",
    "\n",
    "This function estimates the fluctuation parameter ϵ in Targeted Maximum Likelihood Estimation (TMLE) using Overlap Weighting.\n",
    "\n",
    "The idea is to slightly fluctuate (update) the initial outcome regression Q_A by solving a logistic regression with an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aeaf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_fluctuation_param(Y, Q_A, H_1, H_0, A, H_overlap=None):\n",
    "        print(\"📈 Estimating fluctuation parameter (Overlap Weighting)...\")\n",
    "        \n",
    "        Q_A_clipped = np.clip(Q_A, 1e-6, 1 - 1e-6)\n",
    "        logit_QA = np.log(Q_A_clipped / (1 - Q_A_clipped))\n",
    "\n",
    "        if H_overlap is not None:\n",
    "            H_A = H_overlap\n",
    "        else:\n",
    "            H_A = A * H_1 - (1 - A) * H_0\n",
    "            \n",
    "        H_A = H_A.reshape(-1, 1) if H_A.ndim == 1 else H_A\n",
    "\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                model = sm.GLM(Y, H_A, offset=logit_QA, family=sm.families.Binomial()).fit()\n",
    "                eps = model.params[0]\n",
    "                print(f\"   Fluctuation parameter (epsilon): {eps:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   GLM fitting failed, using fallback method: {str(e)}\")\n",
    "            eps = 0.0\n",
    "            \n",
    "        return eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb6a30",
   "metadata": {},
   "source": [
    "### `update_Q `\n",
    "\n",
    "Update Q values using the fluctuation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8040fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Q_base, H, eps):\n",
    "        \n",
    "        Q_clipped = np.clip(Q_base, 1e-6, 1 - 1e-6)\n",
    "        logit_Q = np.log(Q_clipped / (1 - Q_clipped))\n",
    "        updated_Q = 1 / (1 + np.exp(-(logit_Q + eps * H)))\n",
    "        return np.clip(updated_Q, 1e-6, 1 - 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668ab36",
   "metadata": {},
   "source": [
    "### `compute_tmle`\n",
    "\n",
    "This function computes **Targeted Maximum Likelihood Estimation (TMLE)** estimates for the **Average Treatment Effect (ATE)** and **Average Treatment effect on the Treated (ATT)**.  \n",
    "It supports both the traditional TMLE formulation and an **overlap weighting** variant.\n",
    "\n",
    "- ##### Function Purpose\n",
    "    - Compute **ATE** and **ATT** using updated outcome regression models (`Q_*_update`).\n",
    "    - Construct the **influence function (IF)** for variance estimation.\n",
    "    - Estimate **standard error (SE)**, **95% confidence interval (CI)**, and **p-value**.\n",
    "    - Handle both **with overlap weighting** and **without overlap weighting** cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tmle(Y, A, Q_A_update, Q_1_update, Q_0_update, H_1, H_0, overlap_weights=None, H_overlap=None):\n",
    "        print(\"🎯 Computing TMLE estimates (ATE, ATT)...\")\n",
    "        \n",
    "        if overlap_weights is not None:\n",
    "            # ATE with Overlap Weighting\n",
    "            ate_numerator = np.mean((Q_1_update - Q_0_update) * overlap_weights)\n",
    "            ate_denominator = np.mean(overlap_weights)\n",
    "            ate = ate_numerator / ate_denominator\n",
    "            \n",
    "            # ATT\n",
    "            treated_idx = (A == 1)\n",
    "            if np.any(treated_idx):\n",
    "                att_numerator = np.mean((Q_1_update[treated_idx] - Q_0_update[treated_idx]) * overlap_weights[treated_idx])\n",
    "                att_denominator = np.mean(overlap_weights[treated_idx])\n",
    "                att = att_numerator / att_denominator if att_denominator > 0 else np.nan\n",
    "            else:\n",
    "                att = np.nan\n",
    "\n",
    "            # Influence function\n",
    "            if H_overlap is not None:\n",
    "                term1 = H_overlap * (Y - Q_A_update)\n",
    "            else:\n",
    "                term1 = (A * H_1 - (1 - A) * H_0) * (Y - Q_A_update)\n",
    "                \n",
    "            term2 = ((Q_1_update - Q_0_update) * overlap_weights - ate * overlap_weights) / ate_denominator\n",
    "            infl_fn = term1 + term2 - ate\n",
    "            \n",
    "        else:\n",
    "            # Fallback: Original ATE & ATT no overlap weighting\n",
    "            ate = np.mean(Q_1_update - Q_0_update)\n",
    "            att = np.mean((Q_1_update - Q_0_update)[A == 1]) if np.any(A == 1) else np.nan\n",
    "            \n",
    "            H_A = A * H_1 - (1 - A) * H_0\n",
    "            infl_fn = H_A * (Y - Q_A_update) + (Q_1_update - Q_0_update) - ate\n",
    "        \n",
    "        # Calculating standard error, 95% CI, p-value\n",
    "        se = np.sqrt(np.var(infl_fn) / len(Y))\n",
    "        ci_low = ate - 1.96 * se\n",
    "        ci_high = ate + 1.96 * se\n",
    "        p_value = 2 * (1 - norm.cdf(abs(ate / se))) if se > 0 else 1.0\n",
    "        \n",
    "        return ate, se, ci_low, ci_high, p_value, infl_fn, att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611456a2",
   "metadata": {},
   "source": [
    "### `diagnostic_checks`\n",
    "This function performs comprehensive diagnostic checks for both the Q-model (outcome regression) and the G-model (propensity score model). It summarizes treatment group balance, predictive performance, calibration quality, and overlap diagnostics for either the training or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d32a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_checks(Y, A, W, Q_A, Q_1, Q_0, g_w, stage=\"First\", is_test=False, model_performance=None):\n",
    "        \n",
    "        data_type = \"Test\" if is_test else \"Train\"\n",
    "        print(f\"\\n=== {stage} {data_type} Diagnostic Checks ===\")\n",
    "        # ========= Treatment group proportion ==========\n",
    "        treatment_prop = A.mean()\n",
    "        print(f\"{data_type} Treatment group proportion: {treatment_prop:.4f}\")\n",
    "\n",
    "        g_treated = g_w[A==1].mean()\n",
    "        g_control = g_w[A==0].mean()\n",
    "        g_overlap = np.minimum(g_w, 1-g_w).mean()\n",
    "\n",
    "        print(f\"{data_type} Propensity Score: Treated Mean: {g_treated:.4f}\")\n",
    "        print(f\"{data_type} Propensity Score: Control Mean: {g_control:.4f}\")\n",
    "        print(f\"{data_type} Overlap Measure: {g_overlap:.4f} (Higher is better)\")\n",
    "\n",
    "        # ========= Q-model performance metrics ==========\n",
    "        y_pred = (Q_A >= 0.5).astype(int)\n",
    "        auc = roc_auc_score(Y, Q_A)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"{data_type} Q model AUC: {auc:.4f}\")\n",
    "        print(f\"{data_type} Q model Precision: {precision:.4f}\")\n",
    "        print(f\"{data_type} Q model Recall: {recall:.4f}\")\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            print(f\"{data_type} Q model F1 Score: {f1:.4f}\")\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "        #========== G-model (propensity score) performance metrics ==========\n",
    "        print(f\"\\n--- G-model Performance ---\")\n",
    "        g_pred = (g_w >= 0.5).astype(int)\n",
    "        g_auc = roc_auc_score(A, g_w)\n",
    "        g_precision = precision_score(A, g_pred, zero_division=0)\n",
    "        g_recall = recall_score(A, g_pred, zero_division=0)\n",
    "        g_brier = brier_score_loss(A, g_w)\n",
    "        \n",
    "        print(f\"{data_type} G model AUC: {g_auc:.4f}\")\n",
    "        print(f\"{data_type} G model Precision: {g_precision:.4f}\")\n",
    "        print(f\"{data_type} G model Recall: {g_recall:.4f}\")\n",
    "        if g_precision + g_recall > 0:\n",
    "            g_f1 = 2 * (g_precision * g_recall) / (g_precision + g_recall)\n",
    "            print(f\"{data_type} G model F1 Score: {g_f1:.4f}\")\n",
    "        else:\n",
    "            g_f1 = 0.0\n",
    "        print(f\"{data_type} G model Brier Score: {g_brier:.4f}\")\n",
    "        # ========== G-model Calibration Metrics ==========\n",
    "        print(f\"\\n--- Calibration Metrics ---\")\n",
    "        if model_performance is not None:\n",
    "            if is_test and 'test_calibration_metrics' in model_performance:\n",
    "                cal_metrics = model_performance['test_calibration_metrics']\n",
    "                print(f\"{data_type} G model Calibration Error: {cal_metrics['calibration_error']:.4f}\")\n",
    "                print(f\"{data_type} G model Brier Score: {cal_metrics['brier_score']:.4f}\")\n",
    "            elif not is_test and 'train_calibration_metrics' in model_performance:\n",
    "                cal_metrics = model_performance['train_calibration_metrics']\n",
    "                print(f\"{data_type} G model Calibration Error: {cal_metrics['calibration_error']:.4f}\")\n",
    "                print(f\"{data_type} G model Brier Score: {cal_metrics['brier_score']:.4f}\")\n",
    "        # ========= Q-model Distribution ==========\n",
    "        print(f\"{data_type} Q_A distribution: min={Q_A.min():.4f}, max={Q_A.max():.4f}, mean={Q_A.mean():.4f}\")\n",
    "        print(f\"{data_type} Q_1 distribution: min={Q_1.min():.4f}, max={Q_1.max():.4f}, mean={Q_1.mean():.4f}\")\n",
    "        print(f\"{data_type} Q_0 distribution: min={Q_0.min():.4f}, max={Q_0.max():.4f}, mean={Q_0.mean():.4f}\")\n",
    "        # ========= Raw ATE Estimate ==========\n",
    "        raw_ate = np.mean(Q_1 - Q_0)\n",
    "        print(f\"{data_type} {stage} Raw ATE estimate: {raw_ate:.6f}\")\n",
    "        # ========= Extreme Propensity Scores ==========\n",
    "        extreme_ps = np.sum((g_w < 0.05) | (g_w > 0.95))\n",
    "        print(f\"{data_type} Extreme propensity score samples: {extreme_ps} ({extreme_ps/len(g_w)*100:.2f}%)\")\n",
    "\n",
    "        # ====== returning diagnostic results ======\n",
    "        diagnostic_results = {\n",
    "             # ====== Treatment group proportion and overlap metrics ======\n",
    "            'treatment_prop': treatment_prop,\n",
    "            'g_treated': g_treated,\n",
    "            'g_control': g_control,\n",
    "            'g_overlap': g_overlap,\n",
    "            # ====== Q-model performance metrics ======\n",
    "            'q_auc': auc,\n",
    "            'q_precision': precision,\n",
    "            'q_recall': recall,\n",
    "            'q_f1': f1,\n",
    "            'raw_ate': raw_ate,\n",
    "            'extreme_ps_count': extreme_ps,\n",
    "            # ====== G-model performance metrics ======\n",
    "            'g_auc': g_auc,\n",
    "            'g_precision': g_precision,\n",
    "            'g_recall': g_recall,\n",
    "            'g_f1': g_f1,\n",
    "            'g_brier': g_brier\n",
    "        }\n",
    "\n",
    "        return diagnostic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6c6bf",
   "metadata": {},
   "source": [
    "### `print_train_test_comparison`\n",
    "This function prints a comprehensive comparison of train vs. test performance for both the Q-model (Outcome Model) and G-model (Propensity Score Model), along with overlap/propensity score quality checks, and overfitting assessment.\n",
    "\n",
    "- Arguments\n",
    "    - train_metrics: dictionary of performance metrics computed on the training set.\n",
    "    - test_metrics: dictionary of performance metrics computed on the test set.\n",
    "    - title: (optional) custom title for the printed report.\n",
    "\n",
    "- Output\n",
    "    - Prints a formatted table of metrics for Q-model and G-model.\n",
    "    - Provides automatic overfitting assessments based on thresholds.\n",
    "    - Prints calibration evaluation if calibration metrics are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_test_comparison(train_metrics, test_metrics, title=\"Model Performance Comparison\"):\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"                    {title}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"Q-MODEL (Outcome Model) Performance:\")\n",
    "        print(f\"{'Metric':<20} {'Train':<12} {'Test':<12} {'Difference':<12} {'Overfitting?':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        q_metrics = ['q_auc', 'q_precision', 'q_recall', 'q_f1']\n",
    "        q_names = ['AUC', 'Precision', 'Recall', 'F1 Score']\n",
    "        \n",
    "        for metric, name in zip(q_metrics, q_names):\n",
    "            train_val = train_metrics.get(metric, 0)\n",
    "            test_val = test_metrics.get(metric, 0)\n",
    "            diff = train_val - test_val\n",
    "            overfitting = \"Yes\" if diff > 0.05 else \"No\"\n",
    "            print(f\"{name:<20} {train_val:<12.4f} {test_val:<12.4f} {diff:+12.4f} {overfitting:<12}\")\n",
    "        \n",
    "        print(\"\\n G-MODEL (Propensity Score Model) Performance:\")\n",
    "        print(f\"{'Metric':<20} {'Train':<12} {'Test':<12} {'Difference':<12} {'Overfitting?':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        g_metrics = ['g_auc', 'g_precision', 'g_recall', 'g_f1', 'g_brier']\n",
    "        g_names = ['AUC', 'Precision', 'Recall', 'F1 Score', 'Brier Score']\n",
    "        \n",
    "        for metric, name in zip(g_metrics, g_names):\n",
    "            train_val = train_metrics.get(metric, 0)\n",
    "            test_val = test_metrics.get(metric, 0)\n",
    "            if metric == 'g_brier':\n",
    "                diff = test_val - train_val\n",
    "                overfitting = \"Yes\" if diff > 0.02 else \"No\"\n",
    "            else:\n",
    "                diff = train_val - test_val\n",
    "                overfitting = \"Yes\" if diff > 0.05 else \"No\"\n",
    "            print(f\"{name:<20} {train_val:<12.4f} {test_val:<12.4f} {diff:+12.4f} {overfitting:<12}\")\n",
    "        \n",
    "        print(\"\\n Overlap & PS Quality: \")\n",
    "        print(f\"{'Metric':<20} {'Train':<12} {'Test':<12} {'Difference':<12}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        overlap_metrics = ['g_overlap', 'extreme_ps_count']\n",
    "        overlap_names = ['Overlap Quality', 'Extreme PS Count']\n",
    "        \n",
    "        for metric, name in zip(overlap_metrics, overlap_names):\n",
    "            train_val = train_metrics.get(metric, 0)\n",
    "            test_val = test_metrics.get(metric, 0)\n",
    "            diff = train_val - test_val\n",
    "            if metric == 'extreme_ps_count':\n",
    "                print(f\"{name:<20} {train_val:<12.0f} {test_val:<12.0f} {diff:+12.0f}\")\n",
    "            else:\n",
    "                print(f\"{name:<20} {train_val:<12.4f} {test_val:<12.4f} {diff:+12.4f}\")\n",
    "        \n",
    "        # ========= Overfitting Assessment ==========\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"                         OVERFITTING ASSESSMENT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # =========Q-model overfitting check =========\n",
    "        q_auc_diff = train_metrics.get('q_auc', 0) - test_metrics.get('q_auc', 0)\n",
    "        q_f1_diff = train_metrics.get('q_f1', 0) - test_metrics.get('q_f1', 0)\n",
    "        \n",
    "        if q_auc_diff > 0.1 or q_f1_diff > 0.1:\n",
    "            q_assessment = \"🔴 Significant Q-model overfitting detected\"\n",
    "        elif q_auc_diff > 0.05 or q_f1_diff > 0.05:\n",
    "            q_assessment = \"🟡 Moderate Q-model overfitting detected\"\n",
    "        else:\n",
    "            q_assessment = \"🟢 Q-model shows good generalization\"\n",
    "        \n",
    "        # ========= G-model overfitting check =========\n",
    "        g_auc_diff = train_metrics.get('g_auc', 0) - test_metrics.get('g_auc', 0)\n",
    "        g_f1_diff = train_metrics.get('g_f1', 0) - test_metrics.get('g_f1', 0)\n",
    "        g_brier_diff = test_metrics.get('g_brier', 0) - train_metrics.get('g_brier', 0)\n",
    "        \n",
    "        if g_auc_diff > 0.1 or g_f1_diff > 0.1 or g_brier_diff > 0.05:\n",
    "            g_assessment = \"🔴 Significant G-model overfitting detected\"\n",
    "        elif g_auc_diff > 0.05 or g_f1_diff > 0.05 or g_brier_diff > 0.02:\n",
    "            g_assessment = \"🟡 Moderate G-model overfitting detected\"\n",
    "        else:\n",
    "            g_assessment = \"🟢 G-model shows good generalization\"\n",
    "        \n",
    "        print(q_assessment)\n",
    "        print(g_assessment)\n",
    "\n",
    "        # ========= Calibration Assessment ==========\n",
    "        if 'train_calibration_metrics' in train_metrics and 'test_calibration_metrics' in test_metrics:\n",
    "            train_ce = train_metrics['train_calibration_metrics']['calibration_error']\n",
    "            test_ce = test_metrics['test_calibration_metrics']['calibration_error']\n",
    "            ce_diff = test_ce - train_ce\n",
    "            \n",
    "            if ce_diff > 0.05:\n",
    "                cal_assessment = \"🔴 Significant calibration degradation on test set\"\n",
    "            elif ce_diff > 0.02:\n",
    "                cal_assessment = \"🟡 Moderate calibration degradation on test set\"\n",
    "            else:\n",
    "                cal_assessment = \"🟢 Calibration maintains well on test set\"\n",
    "            \n",
    "            print(cal_assessment)\n",
    "            print(f\"   Train Calibration Error: {train_ce:.4f}\")\n",
    "            print(f\"   Test Calibration Error: {test_ce:.4f}\")\n",
    "            print(f\"   Degradation: {ce_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d9027",
   "metadata": {},
   "source": [
    "### `print_results`\n",
    "This function prints a comprehensive summary of TMLE (Targeted Maximum Likelihood Estimation) results, integrating effect estimates, calibration assessment, train/test performance comparisons, and interpretation of findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df10efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(ate, se, ci_low, ci_high, p_value, raw_ate, train_diagnostics, test_pre_diagnostics, test_post_diagnostics, att, calibration_info=None):\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"    TMLE Results (Overlap Weighting + Calibrated G-Model + Train/Test Split)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ======== main results table ========\n",
    "        print(f\"{'Estimand':<12} {'Estimate':<12} {'Std.Err':<10} {'95% CI':<25} {'P-value':<10} {'Significant':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'ATE':<12} {ate:<12.6f} {se:<10.6f} [{ci_low:.6f}, {ci_high:.6f}] {p_value:<10.6f} {'Yes' if p_value < 0.05 else 'No':<12}\")\n",
    "        \n",
    "        if not np.isnan(att):\n",
    "            print(f\"{'ATT':<12} {att:<12.6f} {'---':<10} {'---':<25} {'---':<10} {'---':<12}\")\n",
    "        else:\n",
    "            print(f\"{'ATT':<12} {'N/A':<12} {'---':<10} {'---':<25} {'---':<10} {'---':<12}\")\n",
    "\n",
    "        # ======== Calibration information ========\n",
    "        if calibration_info is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"                      CALIBRATION ASSESSMENT\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"📊 Calibration Method: {calibration_info['calibration_method'].title()}\")\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"            Calibration Metrics Comparison (Downsampled Training)\")\n",
    "            print(\"-\"*60)\n",
    "            print(f\"{'Metric':<20} {'Before':<12} {'After':<12} {'Improvement':<12}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            pre_ce = calibration_info['pre_calibration_metrics']['calibration_error']\n",
    "            post_ce = calibration_info['post_calibration_metrics']['calibration_error']\n",
    "            ce_improvement = pre_ce - post_ce\n",
    "            \n",
    "            pre_brier = calibration_info['pre_calibration_metrics']['brier_score']\n",
    "            post_brier = calibration_info['post_calibration_metrics']['brier_score']\n",
    "            brier_improvement = pre_brier - post_brier\n",
    "            \n",
    "            print(f\"{'Calibration Error':<20} {pre_ce:<12.4f} {post_ce:<12.4f} {ce_improvement:+.4f}\")\n",
    "            print(f\"{'Brier Score':<20} {pre_brier:<12.4f} {post_brier:<12.4f} {brier_improvement:+.4f}\")\n",
    "\n",
    "            # ======== Complete training and testing set calibration performance comparison ========    \n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"           Full Train vs Test Set Calibration Performance\")\n",
    "            print(\"-\"*60)\n",
    "            train_ce = calibration_info['train_calibration_metrics']['calibration_error']\n",
    "            train_brier = calibration_info['train_calibration_metrics']['brier_score']\n",
    "            test_ce = calibration_info['test_calibration_metrics']['calibration_error']\n",
    "            test_brier = calibration_info['test_calibration_metrics']['brier_score']\n",
    "            \n",
    "            print(f\"{'Train Set CE':<20} {train_ce:<12.4f}\")\n",
    "            print(f\"{'Test Set CE':<20} {test_ce:<12.4f}\")\n",
    "            print(f\"{'CE Degradation':<20} {test_ce - train_ce:+12.4f}\")\n",
    "            print(f\"{'Train Set Brier':<20} {train_brier:<12.4f}\")\n",
    "            print(f\"{'Test Set Brier':<20} {test_brier:<12.4f}\")\n",
    "            print(f\"{'Brier Degradation':<20} {test_brier - train_brier:+12.4f}\")\n",
    "\n",
    "            # ====== Adding training diagnostics information to calibration information =======\n",
    "            if 'train_calibration_metrics' in calibration_info:\n",
    "                train_diagnostics.update({\n",
    "                    'train_calibration_metrics': calibration_info['train_calibration_metrics']\n",
    "                })\n",
    "            if 'test_calibration_metrics' in calibration_info:\n",
    "                test_post_diagnostics.update({\n",
    "                    'test_calibration_metrics': calibration_info['test_calibration_metrics']\n",
    "                })\n",
    "            \n",
    "            print(\"\\n📈 Calibration Assessment:\")\n",
    "            if ce_improvement > 0.01:\n",
    "                print(\"   ✅ Significant improvement in training calibration error\")\n",
    "            elif ce_improvement > 0:\n",
    "                print(\"   ✓ Slight improvement in training calibration error\")\n",
    "            else:\n",
    "                print(\"   ⚠️  No improvement in training calibration error\")\n",
    "            # ======== Test Set Calibration Assessment ========\n",
    "            if test_ce < 0.05:\n",
    "                print(\"   ✅ Excellent test set calibration (CE < 0.05)\")\n",
    "            elif test_ce < 0.10:\n",
    "                print(\"   ✓ Good test set calibration (CE < 0.10)\")\n",
    "            else:\n",
    "                print(\"   ⚠️  Poor test set calibration (CE ≥ 0.10)\")\n",
    "\n",
    "        # ======== Comprehensive train/test performance comparison ========\n",
    "        print_train_test_comparison(train_diagnostics, test_post_diagnostics, \"COMPREHENSIVE TRAIN/TEST PERFORMANCE COMPARISON\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"                          INTERPRETATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        # ======== ATE interpretations ========\n",
    "        print(\"🎯 ATE (Average Treatment Effect):\")\n",
    "        print(\"   - Population-wide average causal effect\")\n",
    "        print(\"   - Uses overlap weighting and calibrated propensity scores\")\n",
    "        print(f\"   - Estimate: {ate:.6f}\")\n",
    "        # ======== ATT interpretations ========\n",
    "        print(\"\\n🎪 ATT (Average Treatment Effect on the Treated):\")\n",
    "        print(\"   - Average causal effect among those who received treatment\")\n",
    "        print(\"   - Policy-relevant for understanding treatment effectiveness\")\n",
    "        if not np.isnan(att):\n",
    "            print(f\"   - Estimate: {att:.6f}\")\n",
    "        else:\n",
    "            print(\"   - Not available (no treated units)\")\n",
    "        # ======== Raw vs TMLE Comparison ========\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"            Raw vs TMLE Comparison (Test Set)\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"Raw ATE (Pre-update):     {test_pre_diagnostics['raw_ate']:.6f}\")\n",
    "        print(f\"TMLE ATE (Post-update):   {ate:.6f}\")\n",
    "        print(f\"Adjustment Magnitude:     {abs(ate - test_pre_diagnostics['raw_ate']):.6f}\")\n",
    "        if test_pre_diagnostics['raw_ate'] != 0:\n",
    "            relative_change = abs(ate - test_pre_diagnostics['raw_ate'])/abs(test_pre_diagnostics['raw_ate'])*100\n",
    "            print(f\"Relative Change:          {relative_change:.2f}%\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # ======== Effect Size Interpretation ========\n",
    "        if abs(ate) < 0.01:\n",
    "            effect_size = \"Negligible\"\n",
    "        elif abs(ate) < 0.05:\n",
    "            effect_size = \"Small\"\n",
    "        elif abs(ate) < 0.1:\n",
    "            effect_size = \"Medium\"\n",
    "        else:\n",
    "            effect_size = \"Large\"\n",
    "\n",
    "        direction = \"Positive\" if ate > 0 else \"Negative\"\n",
    "        significance = \"Statistically Significant\" if p_value < 0.05 else \"Not Statistically Significant\"\n",
    "\n",
    "        print(f\"📊 Effect Summary: {effect_size} {direction} Treatment Effect, {significance}\")\n",
    "        print(\"   🎯 Enhanced with calibrated propensity scores for improved reliability\")\n",
    "        print(\"   📈 Comprehensive train/test performance comparison provided\")\n",
    "        print(\"   🔍 Overfitting assessment included for model validation\")\n",
    "        print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6aa86",
   "metadata": {},
   "source": [
    "# `` tmle_project``\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmle_project(file_path, test_size=0.3, random_state=42, calibration_method='platt'):\n",
    "    print(\"***** Start TMLE Analysis with Calibrated G-Model & Train/Test Comparison *****\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\" Using {calibration_method} calibration method\")\n",
    "\n",
    "    with tqdm(total=12, desc=\"TMLE Progress\", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as main_pbar:\n",
    "\n",
    "        try:\n",
    "            # 1. Data Loading and Splitting\n",
    "            main_pbar.set_description(\"====== Loading and Splitting Data ======\")\n",
    "            data_splits = data_loading(file_path, test_size, random_state)\n",
    "            main_pbar.update(1)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            # 2. Data Preprocessing\n",
    "            main_pbar.set_description(\"======  Preprocessing Data =======\")\n",
    "            W_train_std, W_test_std, scaler = data_preprocessing(\n",
    "                data_splits['train']['W'], data_splits['test']['W']\n",
    "            )\n",
    "            \n",
    "            W_A_train_std = pd.concat([W_train_std, pd.DataFrame(data_splits['train']['A'], columns=['A'])], axis=1)\n",
    "            W_A_test_std = pd.concat([W_test_std, pd.DataFrame(data_splits['test']['A'], columns=['A'])], axis=1)\n",
    "            main_pbar.update(1)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            # 3. Set up base learners\n",
    "            main_pbar.set_description(\"====== Setting Up Base Learners ======\")\n",
    "            base_learners = get_base_learners(W_train_std.shape[1])\n",
    "            main_pbar.update(1)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            # 4. Fit Q models (on training set)\n",
    "            main_pbar.set_description(\"======= Step 1: Fit Outcome Models (Q) on Train Set ======\")\n",
    "            sl = fit_superlearner(W_A_train_std, data_splits['train']['Y'], base_learners, \"Outcome Model\")\n",
    "            \n",
    "            # 5. Predict Q models (on both train and test sets)\n",
    "            Q_A_train, Q_1_train, Q_0_train = predict_Q_models(sl, W_A_train_std, is_test=False)\n",
    "            Q_A_test, Q_1_test, Q_0_test = predict_Q_models(sl, W_A_test_std, is_test=True)\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 6.  Estimate calibrated propensity scores (g) (on both train and test sets)\n",
    "            main_pbar.set_description(\"====== Step 2: Estimate Calibrated Propensity Scores (g) ======\")\n",
    "            g_results = estimate_g_with_calibration(\n",
    "                data_splits['train']['A'], W_train_std, \n",
    "                data_splits['test']['A'], W_test_std, \n",
    "                base_learners, calibration_method\n",
    "            )\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 7. Training set diagnostic checks\n",
    "            main_pbar.set_description(\"====== Diagnostic Checks : Training Set ======\")\n",
    "            train_diagnostics = diagnostic_checks(\n",
    "                data_splits['train']['Y'], data_splits['train']['A'], data_splits['train']['W'], \n",
    "                Q_A_train, Q_1_train, Q_0_train, g_results['train']['g_w'], \"Training\", is_test=False, \n",
    "                model_performance=g_results['calibration_info']\n",
    "            )\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 8. Pre-update diagnostic checks (testing)\n",
    "            main_pbar.set_description(\"====== Diagnostic Checks : Pre-update (Test Set) ======\")\n",
    "            test_pre_diagnostics = diagnostic_checks(\n",
    "                data_splits['test']['Y'], data_splits['test']['A'], data_splits['test']['W'], \n",
    "                Q_A_test, Q_1_test, Q_0_test, g_results['test']['g_w'], \"Pre-update\", is_test=True, \n",
    "                model_performance=g_results['calibration_info']\n",
    "            )\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 9. Estimate and update fluctuation parameters (testing)\n",
    "            main_pbar.set_description(\"====== Step 3: TMLE Update (Test Set) ======\")\n",
    "            eps = estimate_fluctuation_param(\n",
    "                data_splits['test']['Y'], Q_A_test, g_results['test']['H_1'], g_results['test']['H_0'], \n",
    "                data_splits['test']['A'], g_results['test']['H_overlap']\n",
    "            )\n",
    "\n",
    "            # Q function updates (testing)\n",
    "            Q_A_update_test = update_Q(Q_A_test, g_results['test']['H_overlap'], eps)\n",
    "            Q_1_update_test = update_Q(Q_1_test, (1 - g_results['test']['g_w']), eps)\n",
    "            Q_0_update_test = update_Q(Q_0_test, (-g_results['test']['g_w']), eps)\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 10. Post-update diagnostic checks (testing)\n",
    "            main_pbar.set_description(\"====== Diagnostic Checks : Post-update (Test Set) ======\")\n",
    "            test_post_diagnostics = diagnostic_checks(\n",
    "                data_splits['test']['Y'], data_splits['test']['A'], data_splits['test']['W'], \n",
    "                Q_A_update_test, Q_1_update_test, Q_0_update_test, g_results['test']['g_w'], \"Post-update\", is_test=True,\n",
    "                model_performance=g_results['calibration_info']\n",
    "            )\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 10. Compute final results (testing set)\n",
    "            main_pbar.set_description(\"====== Compute Final Results (Test Set) ======\")\n",
    "            ate, se, ci_low, ci_high, p_value, infl_fn, att = compute_tmle(\n",
    "                data_splits['test']['Y'], data_splits['test']['A'], \n",
    "                Q_A_update_test, Q_1_update_test, Q_0_update_test, \n",
    "                g_results['test']['H_1'], g_results['test']['H_0'], \n",
    "                g_results['test']['overlap_weights'], g_results['test']['H_overlap']\n",
    "            )\n",
    "            \n",
    "            raw_ate = test_pre_diagnostics['raw_ate']\n",
    "            main_pbar.update(1)\n",
    "            \n",
    "            # 11. Print results\n",
    "            main_pbar.set_description(\"====== Printing Results ======\")\n",
    "            print_results(ate, se, ci_low, ci_high, p_value, raw_ate, train_diagnostics, test_pre_diagnostics, test_post_diagnostics, att, g_results['calibration_info'])\n",
    "            main_pbar.update(1)\n",
    "\n",
    "            # 12. Additional Information\n",
    "            main_pbar.set_description(\"====== Final Summary ======\")\n",
    "            print(f\"\\n📊 Dataset Information:\")\n",
    "            print(f\"   Training set size: {len(data_splits['train']['Y'])}\")\n",
    "            print(f\"   Test set size: {len(data_splits['test']['Y'])}\")\n",
    "            print(f\"   Test size ratio: {test_size*100:.1f}%\")\n",
    "            print(f\"   Calibration method: {calibration_method}\")\n",
    "            main_pbar.update(1)\n",
    "            \n",
    "            return {\n",
    "                'ate': ate, \n",
    "                'att': att, \n",
    "                'se': se, \n",
    "                'ci_low': ci_low, \n",
    "                'ci_high': ci_high, \n",
    "                'p_value': p_value, \n",
    "                'raw_ate': raw_ate, \n",
    "                'influence_function': infl_fn,\n",
    "                'train_diagnostics': train_diagnostics,  \n",
    "                'test_pre_diagnostics': test_pre_diagnostics, \n",
    "                'test_post_diagnostics': test_post_diagnostics,\n",
    "                'overlap_weights': g_results['test']['overlap_weights'], \n",
    "                'g_scores': g_results['test']['g_w'],\n",
    "                'calibration_info': g_results['calibration_info'],\n",
    "                'train_size': len(data_splits['train']['Y']), \n",
    "                'test_size': len(data_splits['test']['Y']),\n",
    "                'test_ratio': test_size,\n",
    "                'calibration_method': calibration_method\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            main_pbar.set_description(\"<< Analysis Failed >>\")\n",
    "            print(f\"An error occurred during the analysis: {str(e)}\")\n",
    "            print(\"Please check the data format and path\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f721dbc",
   "metadata": {},
   "source": [
    "### main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # calibration method: 'platt' or 'isotonic' \n",
    "    calibration_methods = ['platt', 'isotonic']\n",
    "    \n",
    "    print(\"🎯 Available calibration methods:\")\n",
    "    print(\"   - 'platt': Platt scaling (sigmoid function)\")\n",
    "    print(\"   - 'isotonic': Isotonic regression (monotonic function)\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    # using Platt scaling as default\n",
    "    results = tmle_project(\n",
    "        '/Users/chendawei/Desktop/MIT TMLE ICU project/yasmeen tmle/tmle_data.csv', \n",
    "        test_size=0.3, \n",
    "        random_state=42,\n",
    "        calibration_method='platt'  \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
